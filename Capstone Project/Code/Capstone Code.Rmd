---
title: "Capstone Code"
author: "Aaron"
date: "6/25/2017"
output: html_document
---

## Load Data and Libraries

```{r, echo=T, message=FALSE, results="hide", warning=FALSE, fig.keep='none'}
# Load Data and Libraries

library(dplyr)
library(tidyr)
library(reshape2)
library(plyr)
jobs_by_sector  <- read.csv("Data/jobs_by_sector.csv")
home_prices     <- read.csv("Data/home_prices.csv")
total_jobs      <- read.csv("Data/total_jobs.csv")
unemployment    <- read.csv("Data/unemployment.csv")
apt_rents       <- read.csv("Data/apt_rents.csv")
mortgagerates   <- read.csv("Data/mortgagerates.csv")
jobs_by_sector  <- tbl_df(jobs_by_sector)
total_jobs      <- tbl_df(total_jobs)
unemployment    <- tbl_df(unemployment)
apt_rents       <- tbl_df(apt_rents)
mortgagerates   <- tbl_df(mortgagerates)

# Bring column names (dates) into a unique column so table is in long format
jobs_by_sector <- melt(jobs_by_sector, variable.name = "Sector")

# Rename column to correct variable name
colnames(jobs_by_sector)[2] <- "Date"

# Move sector names into column names moving the table back to wide format
jobs_by_sector <- spread(jobs_by_sector, Sector, value)

# Rename column names to be short and concise
colnames(jobs_by_sector)[3] <- "Education"
colnames(jobs_by_sector)[4] <- "Finance"
colnames(jobs_by_sector)[6] <- "Hospitality"
colnames(jobs_by_sector)[8] <- "Mining"
colnames(jobs_by_sector)[9] <- "Other"
colnames(jobs_by_sector)[10] <- "Business"
colnames(jobs_by_sector)[11] <- "Public"
colnames(jobs_by_sector)[12] <- "Trade"

# Changing dates from string to date format. as.Date returns NA for "Sept" 
# abbreviation so first we'll change that to "Sep"
jobs_by_sector$Date <- gsub("Sept", "Sep", jobs_by_sector$Date)
# Adding a day
jobs_by_sector$Date <- paste0("01.", jobs_by_sector$Date)
# Format date 
jobs_by_sector$Date <- as.Date(jobs_by_sector$Date, format = "%d.%b.%Y", "%b/%d/%Y")

# Setup column names for gather function to put back into long format
jobs_by_sector <- jobs_by_sector %>% 
  gather(Construction, Education, Finance, Information, Hospitality, 
         Manufacturing, Mining, Other, Business, Public, Trade, Unclassified, 
         key = "JobSector", value = "NumJobs")

# Setting up dates "Sept" replaced with "Sep"
home_prices$Date <- gsub("Sept", "Sep", home_prices$Date)

# Adding a day then format to date
home_prices$Date <- home_prices$Date %>% 
    paste("01", sep = " ") %>% 
    as.Date(format = "%b %Y %d", "%b/%d/%Y")

# Same steps for total_jobs df
total_jobs$Date <- gsub("Sept", "Sep", total_jobs$Date)
total_jobs$Date <- total_jobs$Date %>%
    paste("01", sep = " ") %>% 
    as.Date(format = "%b %Y %d", "%b/%d/%Y")

# Again for unemployment df
unemployment$Date <- gsub("Sept", "Sep", unemployment$Date)
unemployment$Date <- unemployment$Date %>%
    paste("01", sep = " ") %>%
    as.Date(format = "%b %Y %d", "%b/%d/%Y")

# Slightly different strategy for apt_rents df. Dates are in integer format. 
# First I added the day then converted to date format.
apt_rents$Date <- paste0("01/", apt_rents$Date)
apt_rents$Date <- apt_rents$Date %>% 
    as.Date(format = "%d/%m/%Y", "%b/%d/%Y")

# Cleaninging up mortgage rates with more consies variable names then date format
colnames(mortgagerates)[1] <- "Date"
colnames(mortgagerates)[2] <- "Rates"
mortgagerates$Date <- mortgagerates$Date %>% 
    as.Date(format = "%Y-%m-%d", "%b/%d/%Y")

# Combine all data frames by date nd arranging in desending order
clean_df <- join_all(list(jobs_by_sector,apt_rents,home_prices,total_jobs,
                          unemployment, mortgagerates), by="Date", type="left")
clean_df <- arrange(clean_df, desc(Date))

# Cleaning up some formating to convert data types to numeric by removing dollar 
# signs and commas
clean_df$Condo.Townhome <- gsub("\\$|,", "", clean_df$Condo.Townhome)
clean_df$Single.Family.Home <- gsub("\\$|,", "", clean_df$Single.Family.Home)
clean_df$X2.bedroom <- gsub("*,", "", clean_df$X2.bedroom)
clean_df$X1.Bedroom <- gsub("*,", "", clean_df$X1.Bedroom)
clean_df$Average <- gsub("*,", "", clean_df$Average)

# Converting integers to numeric across variables
clean_df[3:9] <- lapply(clean_df[3:9], as.numeric)

# Converting job secotrs from characters to factors
clean_df$JobSector <- as.factor(clean_df$JobSector)

# Renaming variables with more descrptive concise names
colnames(clean_df)[4] <- "AvgAptRent"
colnames(clean_df)[5] <- "Avg1bdAptRent"
colnames(clean_df)[6] <- "Avg2bdAptRent"
colnames(clean_df)[7] <- "AvgPriceHome"
colnames(clean_df)[8] <- "AvgPriceCondo"
colnames(clean_df)[9] <- "TotalJobs"
colnames(clean_df)[10] <- "URateSJ"
colnames(clean_df)[11] <- "URateSJMetro"

# Removing missing data
clean_df <- na.omit(clean_df)

# Adding a month variable for exploritory analyis
month_prices <- c('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 
                  'Oct', 'Nov', 'Dec')
clean_df$Month <- factor(format(clean_df$Date, "%b"), levels = month_prices)

# Create final new csv file
write.csv(clean_df, file = "Data/clean_df.csv")

# Summary of the final data frame
str(clean_df)

```

## Exporitory Analysis

```{r, echo=T, message=FALSE, results="hide", warning=FALSE, fig.keep='none'}
library(ggplot2)
library(ggthemes)
library(tidyr)
library(dplyr)

# Looking at correlations 
cor(clean_df[3:12], use="complete.obs")

# Time series of avergae home prices
ggplot(clean_df, aes(Date, AvgPriceHome/1000)) + geom_point() + geom_smooth() + 
  labs(title = "Price of Average Single Family Home", y = "Price ($1,000s)") + theme_economist()

# Time series of average home prices line, drop off at the end of each year. Best time to buy a home?
ggplot(clean_df, aes(Date, AvgPriceHome/1000)) + geom_line() + 
  labs(title = "Price of Average Single Family Home", y = "Price ($1,000s)") + theme_economist()

# Box plot of average home prices by month
ggplot(clean_df, aes(Month, AvgPriceHome/1000, group=Month, fill=Month)) + geom_boxplot() + 
  theme_economist() + theme(legend.position="none") + labs(title = "Single Family Home Box Plot", y = "Price ($1,000s)")

# Time series of jobs by sector 2008-2016
ggplot(clean_df, aes(Date, NumJobs, col = JobSector)) + geom_line() + labs(y = "Number of Jobs") + theme_economist()

# Box plot of jobs by sector
ggplot(data=clean_df, aes(x=JobSector, y=NumJobs, group=JobSector, fill=JobSector)) + 
  geom_boxplot() + coord_flip() + guides(fill=FALSE) + theme_economist() + 
  labs(x = "Job Sector", y = "Number of Jobs", title = "Job Sector Box Plot")

# Time series of Education sector jobs with visable seasonality
ggplot(subset(clean_df, JobSector=="Education")) + geom_line(aes(Date, NumJobs, col = JobSector)) + 
  theme_economist() + labs(title = "Education Jobs Sector", y = "Number of Jobs") + 
  theme(legend.position="none")

# Histogram of San Jose unemployment rate
ggplot(clean_df, aes(URateSJ, fill = cut(URateSJ, 25))) + geom_histogram(show.legend = FALSE, bins = 25) + 
  scale_fill_discrete(h = c(240, 100)) + theme_economist() + 
  labs(x = "Unemployment Rate (%)", y = "Number of months", title = "Uneployment Rate Histogram")

# Time series of historical unemployment rate
ggplot(clean_df, aes(Date, URateSJ)) + geom_line() + theme_economist() + 
  labs(title = "San Jose Unemployment Rate", y = "Unemployment Rate (%)")

# Time series of average 30-year mortage rates 
ggplot(clean_df, aes(Date, Rates)) + geom_point() + geom_smooth() + theme_economist() + 
  labs(y = "Average 30-year Mortgage Rate (%)")

```

## Model Building

```{r, echo=T, message=FALSE, results="hide", warning=FALSE, fig.keep='none'}
library(rpart)
library(rpart.plot)
library(caTools)

set.seed(123)

# Create random test and training data sets
split <- sample.split(clean_df$AvgPriceHome, SplitRatio = 0.7)
train <- subset(clean_df, split==TRUE)
test <- subset(clean_df, split==FALSE)

# Create linear regression model and determine its accuarcy on our test data set
linereg <- lm(AvgPriceHome~AvgAptRent+URateSJ+Rates, data = train)
linereg.pred <- predict(linereg, newdata = test)
linereg.rmse <- sqrt(mean((linereg.pred - test$AvgPriceHome)^2))
linereg.rmse
# [1] 42637.19

# K-fold cross validation for linear regression model
library(DAAG)
cv.lm(clean_df, linereg, m=3)
# RMSE <- 43458

# Create regression tree model
treereg <- rpart(AvgPriceHome~AvgAptRent+URateSJ+Rates, data = train)
prp(treereg)
treereg.pred <- predict(treereg, newdata = test)
treereg.rmse <- sqrt(mean((treereg.pred - test$AvgPriceHome)^2))
treereg.rmse
# [1] 24843.1

# Cross validation using complexity parameter (cp)
library(caret)
library(e1071)

# Using 10 folds 
tr.control <- trainControl(method="cv", number = 10)

# Create a grid for all cp values to try
cp.grid = expand.grid(.cp=(0:10)*0.001)
treebest <- train(AvgPriceHome~NumJobs+AvgAptRent+Avg1bdAptRent+Avg2bdAptRent+AvgPriceCondo+
                    TotalJobs+URateSJ+URateSJMetro+Rates, data=train, method = "rpart", 
                    trControl = tr.control, tuneGrid = cp.grid)
treebest
# RMSE was used to select the optimal model using  the smallest value.
# The final value used for the model was cp = 0.

treebest <- treebest$finalModel
prp(treebest)
best.tree.pred <- predict(treebest, newdata = test)
best.tree.rmse <- sqrt(mean((best.tree.pred - test$AvgPriceHome)^2))
best.tree.rmse
# [1] 3591.5

# time-series model ARIMA models aim to describe the autocorrelations in the data, housing data is not stationary
library(xts)
clean_df_construct = clean_df[clean_df$JobSector=='Construction',]
#clean_df_construct$Date = as.Date(clean_df_construct$Date, "%m/%d/%Y")
clean_df_construct <- clean_df_construct[(order(as.Date(clean_df_construct$Date))),]
ts <- ts(clean_df_construct$AvgPriceHome, start=c(2009,8), frequency = 12)

ts.train <- window(ts,start = c(2009,8),end = c(2014,12))
ts.test <- window(ts,start = c(2015,1))

# plot the two time series
par(mfrow=c(1,2))
plot(ts.train); plot(ts.test)

# Build Forecast
library(forecast)
set.seed(3978)
# kpss is used to perform time series stationarity test
arima1 <- auto.arima(ts.train, trace = TRUE, test = "kpss", ic="bic", seasonal = T ) 
# The system has chosen ARIMA(0,1,0)(1,0,0)[12] as the best model as it has the lowest BIC.
# Thus, the Auto Regressive and Moving Average components are 0 and
# 1st order differentiation is necessary
# for the seasonal component, The auto regressive value is 1 and everything else is 0

summary(arima1)
# training set RMSE = 22741.76

confint(arima1) # 95% CI of the coefficient of auto regressive component of Seasonal
# between 0.17 and 0.63

#plot diagnostic
par(mfrow=c(1,1))
plot.ts(arima1$residuals)

# As you can see from the plot, the residuals are not fairly distributed around 0. There are
# some seasonal trends,  Overall the time series doesn't appears stationary

# Let us plot acfs and pacfs
par(mfrow=c(1,2))
acf(arima1$residuals,lag.max = 24, main = "acf of the model")

pacf(arima1$residuals,lag.max = 24, main = "pacf of the model")

# As you can see the ACFs and PACFs some times cross the peak limits, so our model has
# auto-correlation and partial auto-correlation problem

# we can verify that from Ljung Box test
Box.test(arima1$residuals,lag = 20, type = "Ljung-Box")

# The null hypothesis of the Ljung Box test is there is no autocorrelation.
# But, p-value is 0.0013, so we reject the null hypothesis

# Let us do normality test
library(tseries)
jarque.bera.test(arima1$residuals)

# Null hypothesis of the jarque bera test is residuals are normally distributed. 
# p-value = 0.4256, so we fail to reject the null hypothesis

# So, overall the model doesn't look that good.
# Let us forecast
arima1.forecast = forecast.Arima(arima1,h=12)
arima1.forecast
par(mfrow=c(1,1))

plot(arima1.forecast,xlab = "Years", ylab = "Avg. Price of Homes")
library(TSPred)
plotarimapred(ts.test,arima1,xlim = c(2015.01,2015.12),range.percent = 0.05)
accuracy(arima1.forecast,ts.test)

# While the model isn't identifying seasonality well, the actual data (dotted black) is
# in the (80% & 95%) confidence interval of the forecast


# Let us assume the model to be multiplicative and take a log of the data
ts <- ts(log10(clean_df_construct$AvgPriceHome), start=c(2009,8), frequency = 12)

ts.train <- window(ts,start = c(2009,8),end = c(2014,12))
ts.test <- window(ts,start = c(2015,1))

# plot the two time series
par(mfrow=c(1,2))
plot(ts.train); plot(ts.test)

# Build Forecast
library(forecast)
set.seed(3978)
# kpss is used to perform time series stationarity test
arima2 <- auto.arima(ts.train, trace = TRUE, test = "kpss", ic="bic", seasonal = T ) 
# The system has chosen ARIMA(0,1,0)(1,1,0)[12] as the best model as it has the lowest BIC.
# Thus, the Auto Regressive and Moving Average components are 0 and
# 1st order differentiation is necessary
# for the seasonal component, The auto regressive value is 1 and differentiated value is 1
summary(arima2)
# training set RMSE = 0.01343 # at a log scale

confint(arima2) # 95% CI of the coefficient of auto regressive component of Seasonal
# between -0.81 and -0.41; don't include 0, so significant

#plot diagnostic
par(mfrow=c(1,1))
plot.ts(arima2$residuals)

# As you can see from the plot, the residuals are not fairly distributed around 0. There are
# some seasonal trends,  Overall the time series doesn't appears stationary

# Let us plot acfs and pacfs
par(mfrow=c(1,2))
acf(arima2$residuals,lag.max = 24, main = "acf of the model")

pacf(arima2$residuals,lag.max = 24, main = "pacf of the model")

# As you can see the ACFs and PACFs some times cross the peak limits, but, the model
# seems much better than arima1

# we can verify that from Ljung Box test
Box.test(arima2$residuals,lag = 20, type = "Ljung-Box")

# The null hypothesis of the Ljung Box test is there is no autocorrelation.
# But, p-value is 0.1282, so we fail to reject the null hypothesis

# Let us do normality test
library(tseries)
jarque.bera.test(arima2$residuals)

# Null hypothesis of the jarque bera test is residuals are normally distributed. 
# p-value = 0.3749, so we fail to reject the null hypothesis

# So, overall the model looks that good.
# Let us forecast
arima2.forecast = forecast.Arima(arima2,h=12)
arima2.forecast
par(mfrow=c(1,1))

plot(arima2.forecast,xlab = "Years", ylab = "Avg. Price of Homes")
library(TSPred)
plotarimapred(ts.test,arima2,xlim = c(2015.01,2015.12),range.percent = 0.05)
accuracy(arima2.forecast,ts.test)

# The model is looking good.

# The root mean squared error and mean absolute error can only be compared between 
# models whose errors are measured in the same units

# Time series data frame to calculate RMSE agains the linear and tree models
clean_df_ts_test <- clean_df_construct[66:77,]
linereg.ts.pred <- predict(linereg, newdata = clean_df_ts_test)
plot(linereg.ts.pred,xlab = "2015 (Months)", ylab = "Avg. Price of Homes", main = "Linear Model Predictions")

tree.ts.pred <- predict(treebest, newdata = clean_df_ts_test)
plot(tree.ts.pred,xlab = "2015 (Months)", ylab = "Avg. Price of Homes", main = "Tree Model Predictions")

```

